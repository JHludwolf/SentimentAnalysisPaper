{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "import os\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, GlobalMaxPool1D, Dropout, Dense\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tweet-sentiment-extraction/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cleaning(text):\n",
    "    text = re.sub(r'https?://www\\.\\S+\\.com','',text)\n",
    "    text = re.sub(r'[^A-Za-z|\\s]','',text)\n",
    "    text = re.sub(r'\\*+','swear',text)                    # Capture swear words that are **** out\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"        #emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"        #symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"        #transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"        #flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_multiplechars(text):\n",
    "    text = re.sub(r'(.)\\1{3,}',r'\\1', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df):\n",
    "    for col in ['text']:#,'selected_text']:\n",
    "        df[col] = df[col].astype(str).apply(lambda x:basic_cleaning(x))\n",
    "        df[col] = df[col].astype(str).apply(lambda x:remove_emoji(x))\n",
    "        df[col] = df[col].astype(str).apply(lambda x:remove_html(x))\n",
    "        df[col] = df[col].astype(str).apply(lambda x:remove_multiplechars(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=128):\n",
    "    tokenizer.enable_truncation(max_length=maxlen)\n",
    "    tokenizer.enable_padding(length=maxlen)\n",
    "    all_ids = []\n",
    "    \n",
    "    for i in range(0, len(texts), chunk_size):\n",
    "        text_chunk = texts[i:i+chunk_size].tolist()\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "    \n",
    "    return np.array(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_news(df,stop,n=1,col='text'):\n",
    "    '''Function to preprocess and create corpus'''\n",
    "    new_corpus=[]\n",
    "    stem=PorterStemmer()\n",
    "    lem=WordNetLemmatizer()\n",
    "    for text in df[col]:\n",
    "        words=[w for w in word_tokenize(text) if (w not in stop)]\n",
    "       \n",
    "        words=[lem.lemmatize(w) for w in words if(len(w)>n)]\n",
    "     \n",
    "        new_corpus.append(words)\n",
    "        \n",
    "    new_corpus=[word for l in new_corpus for word in l]\n",
    "    return new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df_clean = clean(df)\n",
    "df_clean_selection = df_clean.sample(frac=1)\n",
    "X = df_clean_selection.text.values\n",
    "y = pd.get_dummies(df_clean_selection.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(list(X))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(X)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=30522, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=True, wordpieces_prefix=##)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")  \n",
    "# Save the loaded tokenizer locally\n",
    "save_path = './kaggle/working/distilbert_base_uncased/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "# Reload it with the huggingface tokenizers library\n",
    "fast_tokenizer = BertWordPieceTokenizer(\n",
    "                 './kaggle/working/distilbert_base_uncased/vocab.txt', lowercase=True)\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = fast_encode(df_clean_selection.text.astype(str),\n",
    "                fast_tokenizer,\n",
    "                maxlen=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_projector', 'vocab_transform', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "transformer_layer = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "embedding_size = 128\n",
    "input_ = Input(shape=(100,))\n",
    "inp = Input(shape=(128, ))\n",
    "embedding_matrix=transformer_layer.weights[0].numpy()\n",
    "x = Embedding(embedding_matrix.shape[0],\n",
    "              embedding_matrix.shape[1],\n",
    "              embeddings_initializer=Constant(embedding_matrix),\n",
    "              trainable=False)(inp)\n",
    "x = Bidirectional(LSTM(50, return_sequences=True))(x)\n",
    "x = Bidirectional(LSTM(25, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(50, activation='relu', kernel_regularizer='L1L2')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(3, activation='softmax')(x)\n",
    "model_DistilBert = Model(inputs=[inp], outputs=x)\n",
    "model_DistilBert.compile(loss='categorical_crossentropy',\n",
    "                              optimizer='adam',\n",
    "                              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "773/773 [==============================] - 146s 180ms/step - loss: 1.0768 - accuracy: 0.4141 - val_loss: 0.9665 - val_accuracy: 0.5022\n",
      "Epoch 2/10\n",
      "773/773 [==============================] - 144s 186ms/step - loss: 0.8265 - accuracy: 0.6257 - val_loss: 0.6943 - val_accuracy: 0.7143\n",
      "Epoch 3/10\n",
      "773/773 [==============================] - 144s 186ms/step - loss: 0.6825 - accuracy: 0.7183 - val_loss: 0.6260 - val_accuracy: 0.7464\n",
      "Epoch 4/10\n",
      "773/773 [==============================] - 143s 185ms/step - loss: 0.6409 - accuracy: 0.7415 - val_loss: 0.6164 - val_accuracy: 0.7471\n",
      "Epoch 5/10\n",
      "773/773 [==============================] - 138s 178ms/step - loss: 0.6140 - accuracy: 0.7565 - val_loss: 0.6050 - val_accuracy: 0.7434\n",
      "Epoch 6/10\n",
      "773/773 [==============================] - 143s 185ms/step - loss: 0.5943 - accuracy: 0.7617 - val_loss: 0.5939 - val_accuracy: 0.7529\n",
      "Epoch 7/10\n",
      "773/773 [==============================] - 151s 195ms/step - loss: 0.5794 - accuracy: 0.7693 - val_loss: 0.5905 - val_accuracy: 0.7613\n",
      "Epoch 8/10\n",
      "773/773 [==============================] - 147s 190ms/step - loss: 0.5556 - accuracy: 0.7794 - val_loss: 0.5990 - val_accuracy: 0.7580\n",
      "Epoch 9/10\n",
      "773/773 [==============================] - 137s 177ms/step - loss: 0.5397 - accuracy: 0.7838 - val_loss: 0.5988 - val_accuracy: 0.7493\n",
      "Epoch 10/10\n",
      "773/773 [==============================] - 125s 162ms/step - loss: 0.5221 - accuracy: 0.7940 - val_loss: 0.6077 - val_accuracy: 0.7562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa6a507ca90>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_DistilBert.fit(X, y, batch_size=32, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_final = df_clean.sample(frac=1)\n",
    "X_train = fast_encode(df_clean_selection.text.astype(str), fast_tokenizer, maxlen=128)\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Joe/opt/anaconda3/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "859/859 [==============================] - 142s 156ms/step - loss: 0.5149 - accuracy: 0.7983\n",
      "Epoch 2/10\n",
      "859/859 [==============================] - 132s 154ms/step - loss: 0.4954 - accuracy: 0.8057\n",
      "Epoch 3/10\n",
      "859/859 [==============================] - 149s 174ms/step - loss: 0.4800 - accuracy: 0.8124\n",
      "Epoch 4/10\n",
      "859/859 [==============================] - 146s 170ms/step - loss: 0.4616 - accuracy: 0.8222\n",
      "Epoch 5/10\n",
      "859/859 [==============================] - 136s 158ms/step - loss: 0.4450 - accuracy: 0.8277\n",
      "Epoch 6/10\n",
      "859/859 [==============================] - 2028s 2s/step - loss: 0.4208 - accuracy: 0.8362\n",
      "Epoch 7/10\n",
      "859/859 [==============================] - 131s 153ms/step - loss: 0.4009 - accuracy: 0.8462\n",
      "Epoch 8/10\n",
      "859/859 [==============================] - 136s 159ms/step - loss: 0.3821 - accuracy: 0.8543\n",
      "Epoch 9/10\n",
      "859/859 [==============================] - 134s 156ms/step - loss: 0.3583 - accuracy: 0.8649\n",
      "Epoch 10/10\n",
      "859/859 [==============================] - 134s 156ms/step - loss: 0.3385 - accuracy: 0.8727\n"
     ]
    }
   ],
   "source": [
    "Adam_name = Adam(lr=0.001)\n",
    "model_DistilBert.compile(loss='categorical_crossentropy',optimizer=Adam_name,metrics=['accuracy'])\n",
    "history = model_DistilBert.fit(X_train,y_train,batch_size=32,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./tweet-sentiment-extraction/test.csv')\n",
    "df_test.dropna(inplace=True)\n",
    "df_clean_test = clean(df_test)\n",
    "X_test = fast_encode(df_clean_test.text.values.astype(str),\n",
    "                     fast_tokenizer,\n",
    "                     maxlen=128)\n",
    "y_test = df_clean_test.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - 7s 63ms/step\n",
      "The final model shows 0.75 accuracy on the test set.\n"
     ]
    }
   ],
   "source": [
    "y_preds = model_DistilBert.predict(X_test)\n",
    "y_predictions = pd.DataFrame(y_preds,\n",
    "                             columns=['negative','neutral','positive'])\n",
    "y_predictions_final = y_predictions.idxmax(axis=1)\n",
    "accuracy = accuracy_score(y_test,y_predictions_final)\n",
    "print(f\"The final model shows {accuracy:.2f} accuracy on the test set.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c80a7e3a2de4dde693dabf38008612d40fcac8e48a425c55413d53e2fda1a28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
